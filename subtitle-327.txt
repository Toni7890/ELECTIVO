[MÚSICA] Hola y bienvenidos a Python para todos. Estamos repasando un poco el código y, si quieres, puedes acceder al código de muestra y descargarlo también para que puedas repasarlo tú mismo. Lo que vamos a ver hoy es el código de clasificación de la página. Entonces, el código de clasificación de la página, permíteme hacerme una idea del código de clasificación de la página aquí arriba. Esta es la imagen del código de clasificación de la página. Por lo tanto, el código de clasificación de la página tiene cuatro fragmentos de código que se van a ejecutar, cinco fragmentos de código que se van a ejecutar. El primero que vamos a ver es el código araña y luego analizaremos por separado a estos otros tipos más adelante. El primero que veremos es el de las arañas, y de nuevo sigue el mismo patrón: tenemos algunas cosas en la web, en este caso páginas web. Vamos a tener una base de datos que, en cierto modo, simplemente capture las cosas. No está intentando ser particularmente inteligente, pero va a analizarlos con BeautifulSoup y añadir cosas a la base de datos, ¿vale? Entonces, hablaremos sobre cómo ejecutamos el algoritmo de clasificación de la página y, luego, cómo visualizamos el algoritmo de clasificación de la página en un momento. Lo primero que notaré es que puse el código de BeautifulSoup justo aquí, ¿de acuerdo? Así que puedes obtenerlo del archivo bs4.zip. Puede que incluso haya un archivo README, no, pero hay un archivo README en alguna parte. Pero tienes que usar BeautifulSoup, tienes que poner este zip de bs4 o tienes que instalar BeautifulSoup como código fuente. Así que proporciono este zip de bs4 como una forma rápida y sucia si no puedes instalar algo para todos los usuarios de Python en tu sistema. Así que eso es lo que se supone que debe tener. Se supone que tienes que descomprimirlo aquí mismo, en estos archivos. Y no sé qué significa damnit.py. Viene de Beautiful Soup. Si nos fijamos, está en su código fuente. Así que no estoy diciendo groserías. La gente jura que es una sopa hermosa. Lo siento, me disculpo, está bien. Así que el código con el que más vamos a jugar es el de este primero que se llama spider.py. Y vamos a crear bases de datos, vamos a leer las URL y las vamos a analizar con Beautiful Soup, ¿de acuerdo? Y entonces, lo que vamos a hacer es crear un archivo. De nuevo, esto creará spider.sqlite, y aquí estamos en el ranking de páginas, Ls menos l. Spider.sqlite no está allí, así que esto va a crear la base de datos. CREAMOS UNA TABLA. SI NO EXISTE, TENDREMOS UNA CLAVE PRINCIPAL ENTERA, PORQUE AQUÍ USAREMOS CLAVES FORÁNEAS. Vamos a tener una URL y la URL, que es única. El HTML, que es único si recibimos un error. Y luego, durante la segunda mitad, cuando empecemos a clasificar las páginas, tendremos una clasificación antigua y una nueva. porque la forma en que funciona el ranking de la página es tomar el rango anterior , calcular el nuevo rango y luego reemplazar el nuevo rango con el rango anterior y luego hacerlo una y otra vez. Y luego tendremos una tabla de muchos o muchos puntos que realmente apunten hacia atrás, así que llamo a esto de IB y a IB. Lo hicimos con algunas de las publicaciones de Twitter. Y luego esta web es solo en caso de que tenga más de una web, no hace mucha diferencia. Bien, lo que vamos a hacer es SELECCIONAR el identificador, la URL DE LAS PÁGINAS DONDE EL HTML es NULO, este es nuestro indicador de que una página aún no se ha recuperado y el error es NULO ORDER BY RANDOM. Así que este es nuestro camino, estas cosas tan largas. Y no todo este SQL es completamente estándar, pero este orden aleatorio es muy bueno en sqlite. El límite 1 dice que hay que elegir aleatoriamente un registro de esta base de datos en el que este dato sea verdadero y, a continuación, elegirlo al azar. Luego buscaremos una fila y, si esa fila no es correcta, pediremos una nueva web, una URL de inicio y esto activará las cosas, e insertaremos esta nueva URL. De lo contrario, vamos a reiniciar. Tenemos una fila con la que empezar y, de lo contrario, vamos a ordenar la primacía insertando la URL con la que empezamos e insertándola en ella. Si lo has introducido, solo tienes que ir a drchuck.com, que es un buen punto de partida. Y luego lo que hacemos es clasificar las páginas de esta tabla web para limitar los enlaces. Solo hace enlaces a los sitios a los que tú le digas que haga enlaces y probablemente lo mejor para el ranking de tu página sea quedarse con un solo sitio. De lo contrario, nunca volverás a encontrar el mismo sitio. Si dejas que esto deambule por la web sin rumbo fijo, generalmente me quedo con una web que debería intentar llamar sitios web. Estoy recopilando todos los datos, leo esto y hago una lista de las URL, las URL legítimas, y verás cómo las usamos. Y la web es, cuáles son los sitios legítimos a los que vamos a ir, porque vamos a hacer un bucle, preguntaremos cuántas páginas y buscaremos una página nula. De nuevo, vamos a usar ese orden aleatorio según el límite uno, y luego vamos a coger uno. Vamos a obtener el froid, que es la página desde la que estamos enlazando, y luego la URL; de lo contrario, no se recuperará nadie. Por lo tanto, el fromid es que cuando comencemos a añadir enlaces a los enlaces de nuestra página, necesitamos saber la página con la que empezamos. Y esa es la clave principal. Veremos cómo se establece esa clave principal en un segundo. Así que, de lo contrario, no tenemos ninguno. Y vamos a imprimir esto, desde el identificador de la URL con la que estamos trabajando. Solo para asegurarnos, vamos a borrar todos los enlaces, porque no han sido recuperados. Vamos a borrar todos los enlaces, los enlaces son la tabla de conexiones que conecta las páginas a las páginas. Y por eso vamos a acabar con eso. Así que vamos a coger esta URL. Vamos a leerlo. No lo estamos decodificando porque estamos usando BeautifulSoup, que compensa el UTF-8, por lo que podemos preguntar. Este es el código de error HTML y hemos comprobado que 200 es un error válido y, si recibimos un error grave, lo indicaremos en la página. Vamos a configurar ese error, vamos a actualizar las páginas. De esa forma no lo recuperaremos nunca más. Básicamente comprobamos si el tipo de contenido es text/html. Recuerda que en http obtienes el tipo de contenido. Solo queremos recuperarlo. Solo queremos buscar los enlaces en las páginas HTML, así que borramos a ese tío si obtenemos un JPEG o algo así. No vamos a recuperar el formato JPEG, y luego nos comprometemos y continuamos. Así que estas son algo así como páginas con las que no queríamos meternos. Luego imprimimos cuántos caracteres tenemos y los analizamos. Hacemos todo esto en un bloque de prueba y aceptación porque muchas cosas pueden salir mal aquí. Es un bloqueo de intento de aceptación un poco largo. KeyboardInterrupt, eso es lo que ocurre cuando presiono el control + c en mi teclado o el control + z en Windows. Alguna otra excepción probablemente signifique que BeautifulSoup explotó o que algo más explotó. Indicamos con el error=-1 esa URL para no volver a recuperarla. En este punto, en la línea 103, tenemos el HTML de esa URL. Así que lo insertaremos y estableceremos el ranking de la página en 1. Así que la forma en que funciona el ranking de páginas es dar a todas las páginas un valor normal y luego lo altera. Lo veremos en un momento. Así que lo establece con uno. Vamos a insertarlo o ignorarlo. Eso es solo en caso de que estas páginas ya estén, es porque las páginas no están allí. Y luego vamos a hacer una actualización, y eso es como hacer lo mismo dos veces, doblándolo para asegurarnos de que ya está ahí. Si la insertamos o la ignoramos, no haremos nada, y la actualización hará que la conservemos y la confirmemos de forma que, si seleccionamos más adelante, obtendremos esa información. Ahora bien, este código es similar. Recuerda que usamos BeautifulSoup para extraer todas las etiquetas de anclaje. Tenemos un bucle de forma. Sacamos el href. Y verás que este código es un poco [RISA] más complejo que algunos de los anteriores. Porque tiene que ver con la verdadera maldad o imperfección de la web. Por lo tanto, vamos a usar urlparse, que en realidad forma parte del código lib de la URL, y eso dividirá la URL en partes. Regresa, usa urlparse. Tenemos el esquema que es HTTP o HTTPS. Si es una referencia relativa [inaudible]. Todas estas son referencias relativas tomando la URL actual y conectándola. Urljoin sabe de barras y todas esas otras cosas. Comprobamos si hay un ancla, el signo de almohadilla al final de una URL, y desechamos todo lo que haya pasado, incluido el ancla. Si tenemos un archivo JPEG, PNG o GIF, lo vamos a omitir. No queremos preocuparnos por eso. Estamos viendo los enlaces ahora, estamos viendo todos los enlaces. Y si tenemos una barra al final, la cortaremos diciendo -1. Así que esto no es más que cortar y tirar las URL a la basura, que estamos repasando una página y tenemos muchas que no nos gustan, o tenemos que limpiarlas o lo que sea. Y ahora, y las hemos convertido en absolutas, haciendo esto. Es una URL absoluta. Escribes esto de forma lenta pero segura, cuando tu código explota, y lo empiezas de nuevo y lo vuelves a empezar. Luego, lo que hacemos es revisar todas las webs. Recuerda que esas son las URL con las que estamos dispuestos a quedarnos y, por lo general, es solo una. Si esto enlaza con los sitios que nos interesan, lo omitiremos. No nos interesan los enlaces que salen del sitio. Así que esto es como el enlace que salió del sitio, omítelo. Pero ahora, por fin, aquí en la línea 132, estamos listos para ponerlo en las páginas, la URL y el HTML, y está todo bien, ¿verdad? Y, ¿dónde va a quedar ese valor nulo? Ahí mismo, porque no hemos recuperado el HTML. Esto es NULO porque es una página que vamos a recuperar. Le damos a la página un rango de uno y no le damos HTML y, de esa manera, se recuperará y luego lo confirmaremos, ¿de acuerdo? Y luego, queremos obtener la identificación para poder haberlo hecho de una forma u otra. Pero, vamos a hacer una selección para decir, oye, cuál era el identificador que ya estaba ahí o que acababa de crearse. Lo cogemos con un fetchone y decimos recuperar el identificador y ahora vamos a poner un enlace en INSERT OR IGNORE INTO Links. De id a id, que es el id, la clave principal de la página que estamos viendo y buscando longitudes toid es la longitud que acabamos de crear. Y huimos. Así que va a ir y venir e ir y venir. Vamos a ver la declaración de creación que aparece aquí arriba. From_id y to_id están ahí, vale, pues vamos a ejecutarlo. Python3 spider.python, por lo que es nuevo y quiere una URL con la que empezar, y empezaré con mi sitio web favorito, www.dr-chuck.com. Básicamente, este es el primero que pongas, se quedará en este sitio web durante un tiempo, ¿ de acuerdo? Así que pulsaré Enter y cogeré una página solo para yaks. Vale, coge eso e imprímelo diciendo que tiene 8545 caracteres y está impreso que tiene. Seis enlaces, así que si voy a esto y a Open Database, voy al código tres y llego al ranking de páginas y miro esto. Déjame salir para que se cierre. Por lo tanto, fíjese en este diario de SQLite. Eso significa que no ha terminado de cerrarse, así que voy a salir de esto pulsando enter. Y ahora te darás cuenta de que ese archivo de diario desapareció. De lo contrario, no estaríamos obteniendo los datos finales. Ahí vamos, está bien. Así que, Webs, echemos un vistazo a los datos. Webs solo tiene una URL, son las URL que nos permitimos ver. Puedes poner más de una aquí si quieres, pero la mayoría de la gente la deja como una sola. Pages, así que cogimos esta primera y la recuperamos y este es su HTML. Y allí encontramos otras seis URL que son URL de dr-chuck.com, ¿verdad? Había muchas otras URL allí, pero solo encontramos otras cinco, ¿de acuerdo? Y lo que encontraremos es que si vamos a Enlaces, veremos que la página uno enlaza con dos, enlaces con tres, enlaces con cuatro, enlaces con cinco, enlaces con seis. porque la tabla de enlaces no es más que una tabla de muchos a muchos. Así que la página uno apunta a la página dos, la página uno apunta a la página dos, la página uno a la tres, la página uno a la cinco. Bien, eso es lo que pasa cuando tenemos la primera página. Así que recuperemos una página más. Bien, podríamos haber empezado un nuevo rastreo, pero se quedará en dr-chuck.com y solo pediré una página más. Y ahora fue y lo agarró. Lo escogí al azar entre estos tipos nulos, y voy a pulsar enter para cerrarlo. Y luego actualizaré esto y parece que hemos recuperado una muestra de OBI y no hemos recibido ningún enlace nuevo, y por lo tanto la página de enlaces. No, no recibimos ningún enlace nuevo, así que esa página, sea cual fuere la muestra de OBI, no tenía enlaces externos, así que hagamos otra. Una página más. Esa tenía 15 enlaces, así que echémosle un vistazo ahora. Así que ahora, tenemos 15 páginas, eligió esta para hacerlo, ¿verdad? Y ahora ha añadido 15 páginas más. Y luego, si miras los enlaces, verás que la página cuatro, que es una que acaba de recuperar, enlaza con la página uno. Ahora vemos que aquí es donde el ranking de la página va a ser genial. Cuatro enlaces a uno, cuatro enlaces es lo que sea, ya vamos, ¿verdad? Uno va a cuatro, cuatro a uno. Probablemente debería haber puesto una restricción de unicidad a eso. No se supone que haya duplicado eso. Vale, ahora repasemos esto un montón de veces. Así que vamos a publicarlo 100 veces, para 100 páginas. Tardará un minuto. Verás que es como enloquecer en ciertas páginas, no analizarlas. Ya sabes, ha llegado a mi blog. Está encontrando como 27 enlaces. Esta tabla está creciendo a pasos agigantados en este momento. Nos va a llevar un tiempo llegar a los 100, es un poco lento. Lo interesante es que puedo pulsar el control C en cualquier momento [SONIDO], ¿verdad? Y así explotó. Pero está bien porque los datos siguen ahí. Si, por ejemplo, volvemos a las páginas y actualizamos nuestros datos, veremos que tenemos muchísimas cosas. Y esto se reiniciará y todas esas cosas. Así que si buscamos en HTML lo que empecé, verás que hay muchos archivos que tenemos. Y no lo recuperará nunca más porque tienen HTML. Entonces puedo volver a ejecutar esta cosa y ponerla en marcha. Y cuando digo Ctrl+C, su computadora podría fallar, su red podría caerse. Pueden pasar todo tipo de cosas y tú simplemente retomas las cosas por donde terminan. Simplemente comienza donde sale y eso es lo bueno de esto. Vale, así es más o menos cómo funciona esto. Tenemos esta parte en marcha. Estamos viendo cómo fluye hacia spider.sqlite. Estamos viendo que podemos iniciar esto y reemplazarlo. Lo que voy a hacer es volver en el siguiente vídeo y mostraros cómo funcionan todas estas cosas en conjunto y, a continuación, cómo hacemos realmente el posicionamiento de la página. Así que, gracias de nuevo por escucharnos y nos vemos en el próximo vídeo. [MÚSICA]